services:
  ollama:
    image: ollama/ollama
    container_name: ollama
    entrypoint: ""
    command: >
      sh -c " \
        echo '[OLLAMA_SCRIPT] Iniciando servidor Ollama em background...' && \
        ollama serve & \
        SERVER_PID=$$! && \
        echo \"[OLLAMA_SCRIPT] Servidor Ollama iniciado (PID: $$SERVER_PID).\" && \
        echo '[OLLAMA_SCRIPT] Aguardando 15s para o servidor estabilizar...' && \
        sleep 15 && \
        echo '[OLLAMA_SCRIPT] Tentando baixar o modelo llama3...' && \
        ollama pull llama3 && \ # <--- NOME REAL DO MODELO
        echo '[OLLAMA_SCRIPT] Pull do llama3 concluido.' && \
        echo '[OLLAMA_SCRIPT] Tentando baixar o modelo deepseek-llm:7b-chat...' && \
        ollama pull deepseek-llm:7b-chat && \ # <--- NOME REAL DO MODELO
        echo '[OLLAMA_SCRIPT] Pull do deepseek-llm:7b-chat concluido.' && \
        echo '[OLLAMA_SCRIPT] Downloads de modelos finalizados. Listando modelos disponíveis:' && \
        ollama list && \
        echo '[OLLAMA_SCRIPT] Ollama pronto. Aguardando processo do servidor...' && \
        wait $$SERVER_PID || (echo \"[OLLAMA_SCRIPT] Servidor Ollama (PID: $$SERVER_PID) encerrou ou desvinculou. Mantendo container com tail.\" && tail -f /dev/null) \
      "
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama # <--- Definição de volume corrigida
    networks:
      - pasidnet
    healthcheck:
      # IMPORTANTE: Ajuste estes nomes para corresponderem EXATAMENTE ao que 'ollama list' mostra após um pull bem-sucedido
      test: ["CMD-SHELL", "ollama show llama3:latest > /dev/null 2>&1 && ollama show deepseek-llm:7b-chat > /dev/null 2>&1 || exit 1"]
      interval: 20s
      timeout: 15s
      retries: 6 
      start_period: 240s  # Aumentado ainda mais para garantir downloads completos (7B pode ser grande)

  source: # Sem mudanças aqui, apenas para contexto
    build: .
    command: python main.py source
    container_name: source
    depends_on:
      loadbalance1:
        condition: service_started 
      loadbalance2:
        condition: service_started
      ollama: 
        condition: service_healthy 
    networks:
      - pasidnet
    volumes:
      - ./logs:/app/logs

  loadbalance1:
    build: .
    # Exemplo: LB1 usa service1 e service2
    command: python main.py load_balancer 2000 "service1:4001,service2:4002"
    container_name: loadbalance1
    ports:
      - "2000:2000"
    depends_on:
      service1:
        condition: service_started
      service2:
        condition: service_started
    networks:
      - pasidnet
    volumes:
      - ./logs:/app/logs

  loadbalance2:
    build: .
    # Exemplo: LB2 usa service3 e service4
    command: python main.py load_balancer 3000 "service3:4100,service4:4101"
    container_name: loadbalance2
    ports:
      - "3000:3000"
    depends_on:
      service3:
        condition: service_started
      service4:
        condition: service_started
    networks:
      - pasidnet
    volumes:
      - ./logs:/app/logs

  # Exemplo de configuracao para services
  service1:
    build: .
    command: python main.py service 4001 100 llama3.2 # model_name é "llama3.2"
    container_name: service1
    networks:
      - pasidnet
    ports: 
      - "4001:4001" 
    depends_on:
      ollama:
        condition: service_healthy # Espera o healthcheck do ollama (com os modelos) passar
    volumes:
      - ./logs:/app/logs

  service2:
    build: .
    command: python main.py service 4002 100 llama3.2 
    container_name: service2
    networks:
      - pasidnet
    ports:
      - "4002:4002"
    depends_on:
      ollama:
        condition: service_healthy
    volumes:
      - ./logs:/app/logs

  service3:
    build: .
    # Exemplo usando outro modelo para variar o "tipo de servico"
    command: python main.py service 4100 100 deepseek-llm-7b
    container_name: service3
    networks:
      - pasidnet
    ports:
      - "4100:4100"
    depends_on:
      ollama:
        condition: service_healthy
    volumes:
      - ./logs:/app/logs

  service4:
    build: .
    command: python main.py service 4101 100 deepseek-llm-7b
    container_name: service4
    networks:
      - pasidnet
    ports:
      - "4101:4101"
    depends_on:
      ollama:
        condition: service_healthy
    volumes:
      - ./logs:/app/logs

networks:
  pasidnet:
    driver: bridge

volumes:
  ollama_data: {}

    